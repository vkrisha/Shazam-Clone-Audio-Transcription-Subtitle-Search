# Shazam-Clone-Audio-Transcription-Subtitle-Search
ğŸ” Project Overview
 I built an intelligent search engine that allows users to find relevant video subtitles using audio queries. By leveraging vector embeddings and semantic search, this system retrieves the most relevant subtitle segments for any spoken input.
ğŸ”‘ How It Works
âœ… Data Processing
 ğŸ”¹ Processed a 1.9 GB dataset of .srt subtitle files
 ğŸ”¹ Optimized storage by converting data to Parquet format
 ğŸ”¹ Indexed and cleaned 30% of the dataset for efficient search
âœ… Chunking & Embeddings
 ğŸ”¹ Split subtitle files into context-aware chunks
 ğŸ”¹ Generated vector embeddings using ChromaDB + Sentence Transformers
 ğŸ”¹ Leveraged Google Colabâ€™s T4 GPU, completing embedding generation in ~40 minutes
âœ… Search & Retrieval
 ğŸ”¹ Accepted audio files (>2 mins) as input
 ğŸ”¹ Converted speech to text with AssemblyAI
 ğŸ”¹ Encoded text and applied cosine similarity to fetch top 5 relevant subtitle matches
âœ… Deployment & UI
 ğŸ”¹ Faced a few build tool challenges, so deployed on Google Colab + Gradio
 ğŸ”¹ Delivered real-time search with an interactive UI for seamless audio-to-text retrieval
ğŸ’¡ Tech Stack
 ğŸ¹ Automatic Speech Recognition (ASR): AssemblyAI
 ğŸ¹ Vector Database: ChromaDB
 ğŸ¹ NLP Models: Sentence Transformers
 ğŸ¹ Deployment: Google Colab + Gradio
ğŸ“Œ Why Does This Matter?
 This project demonstrates how AI, NLP, and vector search can enhance media accessibility. It has real-world applications in video platforms, transcription services, and AI-driven media indexing.
